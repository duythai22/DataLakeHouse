version: "3.9"

services:
  minio:
    hostname: minio
    image: "minio/minio"
    container_name: minio
    ports:
      - "9001:9001"
      - "9000:9000"
    command: [ "server", "/data", "--console-address", ":9001" ]
    volumes:
      - ./minio/data:/data
    env_file:
      - .env
    networks:
      - data_network
  
  mc:
    image: minio/mc
    container_name: mc
    hostname: mc
    env_file:
      - .env
    entrypoint: >
      /bin/sh -c "  until (/usr/bin/mc config host add minio http://minio:9000/ minio minio123)  do echo '...waiting...' && sleep 10; done; tail -f /dev/null;"
    depends_on:     
      - minio
    networks:
      - data_network

  spark-master:
    build:
      context: ./docker_image/spark
      dockerfile: ./Dockerfile
    container_name: "spark-master"
    ports:
      - "7077:7077"  # Spark master port
      - "8081:8080"  # Spark master web UI port
    expose: 
      - "7077"
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
    volumes:
      - ./docker_image/spark/conf/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
      - ./docker_image/spark/conf/log4j.properties:/opt/bitnami/spark/conf/log4j.properties
    networks:
      - data_network

  spark-worker-1:
    image: docker.io/bitnami/spark:3.3.2
    container_name: "spark-worker-1"
    env_file:
      - .env
    depends_on:
      - spark-master
    networks:
      - data_network

  spark-worker-2:
    image: docker.io/bitnami/spark:3.3.2
    container_name: "spark-worker-2"
    env_file:
      - .env
    depends_on:
      - spark-master
    networks:
      - data_network
  
  spark-notebook:
    build: 
      context: ./notebooks
      dockerfile: ./Dockerfile
    container_name: "spark-notebook"
    user: root
    environment:
      - JUPYTER_ENABLE_LAB="yes"
      - GRANT_SUDO="yes"
    volumes:
      - ./notebooks/work:/home/jovyan/work
      - ./notebooks/conf/spark-defaults.conf:/usr/local/spark/conf/spark-defaults.conf
    ports:
      - "8888:8888"
      - "4040:4040"
    networks:
      - data_network

  # spark-master:
  #   build:
  #     context: ./docker_image/spark
  #     dockerfile: ./Dockerfile
  #   container_name: spark-master
  #   ports:
  #     - "7077:7077"  # Spark master port
  #     - "8081:8080"  # Spark master web UI port 
  #   environment:
  #     - SPARK_MODE=master
  #     - SPARK_RPC_AUTHENTICATION_ENABLED=no
  #     - SPARK_RPC_ENCRYPTION_ENABLED=no
  #     - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
  #     - SPARK_SSL_ENABLED=no
  #     - SPARK_USER=spark
  #     - SPARK_LOCAL_IP=spark-master #edit
  #   expose:
  #     - "7077"
  #   volumes:
  #     # - ".:/opt/spark/"
  #     - ./docker_image/spark/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
  #   networks:
  #     - data_network

  # spark-worker-1:
  #   image: docker.io/bitnami/spark:3.3.2
  #   container_name: "spark-worker-1"
  #   environment:
  #     - SPARK_MODE=worker
  #     - SPARK_MASTER_URL=spark://spark-master:7077
  #     - SPARK_WORKER_MEMORY=1G
  #     - SPARK_WORKER_CORES=1
  #     - SPARK_RPC_AUTHENTICATION_ENABLED=no
  #     - SPARK_RPC_ENCRYPTION_ENABLED=no
  #     - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
  #     - SPARK_SSL_ENABLED=no
  #     - SPARK_USER=spark
  #   # ports:
  #   #   - "8081:8081"  # Spark master web UI port
  #   depends_on:
  #     - spark-master
  #   networks:
  #     - data_network

  # spark-worker-2:
  #   image: docker.io/bitnami/spark:3.3
  #   container_name: "spark-worker-2"
  #   env_file:
  #     - .env
  #   depends_on:
  #     - spark-master
  #   networks:
  #     - data_network

  # metabase:
  #   image: metabase/metabase:latest
  #   container_name: "metabase"
  #   ports:
  #     - "3000:3000"
  #   env_file:
  #     - .env
  #   networks:
  #     - data_network
  
  mysql:
    image: mysql:8.0
    container_name: de_mysql
    volumes:
      - ./storage/mysql_data:/var/lib/mysql
      - ./dataset:/tmp/dataset
      - ./load_dataset_into_mysql:/tmp/load_dataset
    ports:
      - "3307:3306"
    env_file: .env
    networks:
      - data_network

  mariadb:
    image: mariadb:10.5.16
    container_name: mariadb
    # volumes:
    #   - ./mysql:/var/lib/mysql
    ports:
      - "3309:3306"
    env_file:
      - .env
    networks:
      - data_network
  
  hive-metastore:
    container_name: hive-metastore
    hostname: hive-metastore
    image: "bitsondatadev/hive-metastore:latest"
    #entrypoint: /entrypoint.sh
    ports:
      - "9083:9083"
    volumes:
      # - ./hive-metastore/entrypoint.sh:/entrypoint.sh
      - ./docker_image/hive-metastore/metastore-site.xml:/opt/apache-hive-metastore-3.0.0-bin/conf/metastore-site.xml:ro
    environment:
      METASTORE_DB_HOSTNAME: mariadb
    networks:
      - data_network
    depends_on:
      - mariadb
      - minio

  de_psql:
    image: postgres:14-alpine
    container_name: de_psql
    hostname: de_psql
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./pg_hba.conf:/tmp/pg_hba.conf
      # - ./load_dataset:/tmp/load_dataset
    command: ["postgres", "-c", "hba_file=/tmp/pg_hba.conf"]
    expose:
      - "5432"
    ports:
      - "5432:5432"
    env_file: .env
    networks:
      - data_network

  de_dagster_dagit:
    build:
      context: ./docker_image/dagster
      dockerfile: Dockerfile
    entrypoint:
      - dagit
      - -h
      - "0.0.0.0"
      - -p
      - "3001"
      - -w
      - workspace.yaml
    container_name: de_dagster_dagit
    ports:
      - "3001:3001"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./dagster_home:/opt/dagster/dagster_home
    env_file:
      - .env
    networks:
      - data_network
    depends_on:
      - de_psql

  de_dagster_daemon:
    build:
      context: ./docker_image/dagster
      dockerfile: Dockerfile
    entrypoint:
      - dagster-daemon
      - run
    container_name: de_dagster_daemon
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./dagster_home:/opt/dagster/dagster_home
    env_file:
      - .env
    networks:
      - data_network
    depends_on:
      - de_psql

  etl_pipeline:
    build:
      context: ./etl_pipeline
      dockerfile: Dockerfile
    container_name: etl_pipeline
    image: etl_pipeline:latest
    user: root
    volumes:
      - ./etl_pipeline:/opt/dagster/app/etl_pipeline
      # - ./etl_pipeline/spark-defaults.conf:/usr/local/spark/conf/spark-defaults.conf
    env_file:
      - .env
    expose:
      - "4000"
    networks:
      - data_network

networks:
  data_network:
    driver: bridge
    name: data_network

# volumes:
#   spark-data:
#     driver: local

volumes:
  postgres_data: {}